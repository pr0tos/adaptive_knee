# Adaptive Knee Reinforcement Learning Training and Visualization

This guide explains how to set up, train, and visualize a reinforcement learning (RL) using the Soft Actor-Critic (SAC) algorithm. The project simulates a bipedal walker, leveraging the [Myosuite framework](https://myosuite.readthedocs.io/en/latest/).

## Project Setup

### 1. Create and Activate Conda Environment

To ensure compatibility, create a Conda environment with the required dependencies. Use the provided `environment.yml` file if available, or install packages manually.

```bash
# Create Conda environment from YAML file (if available)
conda env create -f environment.yml

# Activate environment
conda activate knee
#install requirements
pip install -r requirements.txt

cd rl_walking
```
### 2. Configuration

Training hyperparameters are defined in `walk_train.py` within the `wandb.init` configuration block. Default values are optimized for `myoLegWalk-v0`:

```python
wandb.init(project="myoLegWalk-training", config={
    "episode_n": 10000,        # Number of training episodes
    "max_steps": 1000,         # Maximum steps per episode
    "gamma": 0.99,             # Discount factor
    "alpha": 0.2,              # Entropy regularization coefficient
    "tau": 0.005,              # Soft update coefficient
    "batch_size": 256,         # Batch size for training
    "pi_lr": 3e-4,             # Policy learning rate
    "q_lr": 3e-4               # Q-network learning rate
})
```

To adjust these parameters, edit the `config` dictionary in `walk_train.py`. All metrics and hyperparameters are logged to Weights & Biases (WandB) for monitoring. In future it will be remake to yaml file (comming soon).

Set up WandB:

```bash
wandb login
```

### 3. Directory Structure

Place the following scripts in the project directory, e.g., `/adaptive_knee/rl_walking/`:
- `sac.py`: Implements the SAC algorithm.
- `walk_train.py`: Trains the SAC agent in the environment.
- `visualize_policy.py`: Visualizes the trained walking policy.

The trained model is saved to `rl_walking/models/sac_policy.pth`.

## Running the Training

The `walk_train.py` script trains the SAC agent to learn stable walking in the `myoLegWalk-v0` environment, a musculoskeletal model of a bipedal walker.

### Steps

1. **Navigate to the project directory**:

   ```bash
   cd adaptive_knee/rl_walking
   ```

2. **Run the training script**:

   ```bash
   python walk_train.py
   ```

3. **What happens**:
   - Initializes the environment and SAC agent.
   - Trains for 10,000 episodes, with up to 1,000 steps per episode (approximately 10 million steps).
   - Logs metrics (total reward, average reward, Q-network losses, policy loss) to WandB.
   - Saves the model to `rl_walking/models/sac_policy.pth` when the average reward over the last 10 episodes improves.
   - Displays a plot of total rewards per episode at the end.

4. **GPU usage**:
   - The script uses GPU if CUDA is available, indicated by `Using device: cuda` in the console.
   - If `Using device: cpu` appears, verify CUDA setup (see Troubleshooting).

5. **Training duration**:
   - Training may take several hours, depending on hardware. A GPU significantly speeds up the process.
   - For testing, reduce `episode_n` (e.g., 100) and `max_steps` (e.g., 200) in `walk_train.py`.

## Running the Visualization

The `visualize_policy.py` script loads the trained model and renders the agent's walking behavior in the `myoLegWalk-v0` environment.

### Steps

1. **Ensure the trained model exists**:
   - Confirm that `rl_walking/models/sac_policy.pth` exists, generated by `walk_train.py`.

2. **Run the visualization script**:

   ```bash
   python walk_viz.py
   ```

3. **What happens**:
   - Loads the model from `rl_walking/models/sac_policy.pth`.
   - Runs the policy for up to 1,000 steps, rendering the environment with MuJoCo.
   - If an episode terminates early (e.g., agent falls), prints the step count and total reward, then resets the environment.
   - Visualization continues until 1,000 steps or manual termination.


